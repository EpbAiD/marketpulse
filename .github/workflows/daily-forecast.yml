name: Daily Market Regime Forecast

on:
  schedule:
    # Run at 6 AM EST daily (11 AM UTC)
    - cron: '0 11 * * *'
  workflow_dispatch:  # Allow manual trigger from Actions tab

jobs:
  forecast:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours - allows for training neural network models
    permissions:
      contents: write  # Allow git push

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      # Models are stored in repo - no artifact download needed!
      - name: Verify models in repository
        run: |
          echo "=============================================="
          echo "Checking models stored in repository..."
          echo "=============================================="

          echo ""
          echo "ðŸ“¦ Core Models:"
          if [ -f "outputs/models/hmm_model.joblib" ]; then
            echo "  âœ“ HMM Model found"
          else
            echo "  âœ— HMM Model missing (will be trained)"
          fi

          if [ -f "outputs/models/regime_classifier.joblib" ]; then
            echo "  âœ“ Regime Classifier found"
          else
            echo "  âœ— Regime Classifier missing (will be trained)"
          fi

          echo ""
          echo "ðŸ“Š Feature Models:"
          MODEL_COUNT=$(find outputs/forecasting/models -name "nf_bundle_v*" -type d 2>/dev/null | wc -l | tr -d ' ')
          VERSION_COUNT=$(find outputs/forecasting/models -name "*_versions.json" -type f 2>/dev/null | wc -l | tr -d ' ')
          ENSEMBLE_COUNT=$(find outputs/forecasting/models -name "*_ensemble_v*.json" -type f 2>/dev/null | wc -l | tr -d ' ')
          echo "  Model bundles: $MODEL_COUNT"
          echo "  Version files: $VERSION_COUNT"
          echo "  Ensemble files: $ENSEMBLE_COUNT (REQUIRED for inference)"

          # Warn if ensemble files are missing
          if [ "$ENSEMBLE_COUNT" -lt "$MODEL_COUNT" ]; then
            echo ""
            echo "  âš ï¸  WARNING: Some ensemble files are missing!"
            echo "     Models without ensemble files will need retraining."
          fi

          echo ""
          echo "=============================================="

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements_github_actions.txt

      - name: Verify critical packages
        run: |
          echo "Verifying critical package installation..."
          python -c "import google.cloud.bigquery; print('âœ“ google-cloud-bigquery installed')"
          python -c "import db_dtypes; print('âœ“ db-dtypes installed')"
          python -c "import pandas; print('âœ“ pandas installed')"
          echo "âœ“ All critical packages verified"

      - name: Create credentials file from secret
        run: |
          echo '${{ secrets.GCP_CREDENTIALS }}' > /tmp/gcp_credentials.json
          echo "GOOGLE_APPLICATION_CREDENTIALS=/tmp/gcp_credentials.json" >> $GITHUB_ENV

      - name: Check if training is needed
        id: check_models
        run: |
          echo "ðŸ” Running intelligent model checker..."

          # Run checker and capture output
          python -m orchestrator.intelligent_model_checker --json > /tmp/model_status.json 2>/dev/null || true

          # Extract workflow recommendation
          WORKFLOW=$(python -c "import json; d=json.load(open('/tmp/model_status.json')); print(d.get('workflow', 'inference'))" 2>/dev/null || echo "inference")
          FEATURES_TO_TRAIN=$(python -c "import json; d=json.load(open('/tmp/model_status.json')); print(','.join(d.get('features_to_train', [])))" 2>/dev/null || echo "")

          echo "Workflow recommendation: $WORKFLOW"
          echo "Features to train: $FEATURES_TO_TRAIN"

          echo "workflow=$WORKFLOW" >> $GITHUB_OUTPUT
          echo "features_to_train=$FEATURES_TO_TRAIN" >> $GITHUB_OUTPUT

      - name: Set up Cloud SDK (for training trigger)
        if: steps.check_models.outputs.workflow != 'inference'
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_CREDENTIALS }}

      - name: Configure gcloud
        if: steps.check_models.outputs.workflow != 'inference'
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}

      - name: Trigger Cloud Run for training (if needed)
        if: steps.check_models.outputs.workflow != 'inference'
        run: |
          echo "ðŸš€ Training needed - triggering Cloud Run GPU job..."
          echo "Workflow: ${{ steps.check_models.outputs.workflow }}"
          echo "Features: ${{ steps.check_models.outputs.features_to_train }}"

          # Trigger Cloud Run job with auto mode
          gcloud run jobs execute marketpulse-training \
            --region=us-central1 \
            --update-env-vars="MODE=auto,FEATURES=${{ steps.check_models.outputs.features_to_train }}" \
            --wait

          echo "âœ… Cloud Run training complete!"

          # Pull the latest models from repo (Cloud Run pushed them)
          git pull origin main

      - name: Run inference pipeline
        env:
          PYTHONUNBUFFERED: 1
        run: |
          echo "Starting daily market regime forecast (inference mode)..."

          # Run inference only (models are ready from repo or Cloud Run)
          python run_pipeline.py --workflow inference --no-clean

          # Final commit of the log
          bash utils/commit_log.sh || true

      # CRITICAL: Commit trained models to repository for permanent storage
      - name: Commit trained models to repository
        if: always()
        run: |
          echo "=============================================="
          echo "Committing trained models to repository..."
          echo "=============================================="

          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "GitHub Actions Bot"

          # Add all model files
          git add outputs/models/*.joblib 2>/dev/null || true
          git add outputs/clustering/*.parquet 2>/dev/null || true
          git add outputs/forecasting/models/ 2>/dev/null || true

          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "No model changes to commit"
          else
            echo "Committing model updates..."
            git commit -m "chore: update trained models [skip ci]"
            git pull --rebase origin main
            git push
            echo "âœ… Models committed to repository!"
          fi

      # Keep artifacts as backup (but models are primarily stored in repo)
      - name: Upload trained models (backup)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: trained-models-backup
          path: |
            outputs/models/*.joblib
            outputs/clustering/*.parquet
            outputs/forecasting/models/*/nf_bundle_v*/**
            outputs/forecasting/models/*/*.json
            outputs/forecasting/models/*_versions.json
          retention-days: 30

      - name: Log daily predictions
        run: |
          # Ensure db-dtypes is available before logging
          python -c "import db_dtypes" || pip install db-dtypes==1.3.1
          python log_daily_predictions.py --output DAILY_PREDICTIONS.md

      - name: Commit and push results
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "GitHub Actions Bot"

          # Add daily predictions and workflow log
          git add DAILY_PREDICTIONS.md || true
          git add WORKFLOW_LOG.md || true

          # Commit if there are changes
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "ðŸ¤– Daily forecast update $(date +'%Y-%m-%d %H:%M UTC')"
            git pull --rebase origin main
            git push
            echo "âœ… Forecast results committed and pushed"
          fi

      - name: Keep Streamlit dashboard awake
        if: success()
        run: |
          echo "Pinging Streamlit dashboard to prevent sleep..."
          curl -I https://marketpulsedashboard.streamlit.app || true
          echo "âœ“ Dashboard pinged successfully"

      - name: Summary
        if: success()
        run: |
          echo "=============================================="
          echo "âœ… Daily forecast completed successfully!"
          echo "=============================================="
          echo "ðŸ“… Timestamp: $(date +'%Y-%m-%d %H:%M UTC')"
          echo "ðŸ’¾ Models: Stored permanently in repository"
          echo "ðŸ”— Results: https://github.com/${{ github.repository }}/blob/main/DAILY_PREDICTIONS.md"
          echo "ðŸ“Š Dashboard: https://marketpulsedashboard.streamlit.app"
          echo "=============================================="
